# AI Intake Assistant - MVP Environment Configuration
# Copy this file to .env and fill in your actual values

# === REQUIRED SETTINGS ===

# Application Port (adjust to avoid conflicts with other Node.js apps)
APP_PORT=3073

# === AI MODEL CONFIGURATION ===
# AI Mode: "static" | "openai" | "ollama"
NEXT_PUBLIC_AI_MODE=static  # Change to "static", "openai", or "ollama"

# OpenAI API Configuration (for openai mode)
OPENAI_API_KEY=
OPENAI_MODEL=gpt-5  # Using GPT-5 model
OPENAI_EMBEDDING_MODEL=text-embedding-3-large

# Ollama Configuration (for ollama mode)
OLLAMA_BASE_URL=http://localhost:11434/v1
OLLAMA_MODEL=gpt-oss:20b  # Local model name
OLLAMA_API_KEY=ollama  # Dummy key for local Ollama

# === OPTIONAL SETTINGS ===

# Database Configuration (for PostgreSQL mode - optional, defaults to CSV)
DB_USE_POSTGRES=false  # Set to true to use PostgreSQL instead of CSV
DB_HOST=localhost
DB_PORT=5432
DB_NAME=ai_intake_db
DB_USER=ai_intake_user
DB_PASSWORD=your_secure_password_here

# Application Settings
NODE_ENV=development  # development or production
SESSION_SECRET=change-this-to-random-string

# File Storage Paths
CSV_DATA_PATH=./data
LOG_PATH=./logs

# Feature Flags
ENABLE_DEBUG_PANEL=true
ENABLE_DECISION_LOGGING=true

# Performance Optimization Flags (Phase 1)
ENABLE_SEMANTIC_DUPLICATE_CHECK=false  # Disable for 10-15s speedup
CONVERSATION_HISTORY_LIMIT=6           # Limit to last 6 messages for 2-4s speedup

# Logging
LOG_LEVEL=info  # error, warn, info, debug

# Docker Settings (only if using Docker)
DOCKER_NETWORK=ai_intake_network